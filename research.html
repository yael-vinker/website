<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=0.5, shrink-to-fit=no">
  
  <title>Yael Vinker</title>
  <meta content="" name="descriptison">
  <meta content="" name="keywords">

  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.4/css/all.css" integrity="sha384-DyZ88mC6Up2uqS4h/KRgHuoeGwBcD4Ng9SiP4dIRy0EXTlnuz47vAwmeGwVChigm" crossorigin="anonymous"/> 
  <!-- Custom fonts for this theme -->
  <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Dosis:400,700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">

  <!-- Favicons -->
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
  <link rel="icon" href="favicon.ico" type="image/x-icon">

  <!-- Theme CSS -->
  <link href="css/freelancer.css" rel="stylesheet">

</head>

<body id="page-top">

  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg bg-secondary fixed-top" id="mainNav">
    <div class="container">
      <a class="navbar-brand js-scroll-trigger" href="index.html">Yael Vinker</a>
      <button class="navbar-toggler navbar-toggler-right text-uppercase bg-secondary text-white rounded" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
        Menu
        <i class="fas fa-bars"></i>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item mx-0 mx-lg-1">
            <a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger large" href="art.html">Art</a>
          </li>
          <li class="nav-item mx-0 mx-lg-1">
            <a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger large" href="pub.html">Research</a>
          </li>
          <li class="nav-item mx-0 mx-lg-1">
            <a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger large" href="research.html">Publications</a>
          </li>
           <li class="nav-item mx-0 mx-lg-1">
            <a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger large" href="talks.html">Talks</a>
          </li>
          <li class="nav-item mx-0 mx-lg-1">
            <a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger large" href="awards.html">Awards</a>
          </li>
          <li class="nav-item mx-0 mx-lg-1">
            <a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger large" href="Yael_Vinker_resume.pdf">CV</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  

  <!-- Portfolio Section -->
  <section class="page-section portfolio" id="portfolio">
    <div class="container">

      <!-- Portfolio Section Heading -->
      <h1 class="page-section-heading text-center mb-6">Selected Publications</h1>

      <svg data-name="Layer 1" id="Layer_1" viewBox="-70 0 600 120" xmlns="http://www.w3.org/2000/svg">
        <defs>
        <style>
              .cls-1 {
                stroke: #fb8c00;
              }
        
              .cls-1, .cls-2, .cls-3, .cls-4, .cls-5, .cls-6 {
                stroke-linecap: round;
              }
        
              .cls-1, .cls-3, .cls-4, .cls-5, .cls-6 {
                fill: none;
                stroke-miterlimit: 2.16;
                stroke-width: 6px;
              }
        
              .cls-2, .cls-7 {
                fill: #fff;
                stroke: #000;
                stroke-miterlimit: 10;
                stroke-width: .5px;
              }
        
              .cls-3 {
                stroke: #757575;
              }
        
              .cls-8 {
                fill: #43a047;
              }
        
              .cls-8, .cls-9, .cls-10, .cls-11, .cls-12, .cls-13, .cls-14 {
                isolation: isolate;
              }
        
              .cls-8, .cls-10, .cls-12, .cls-13, .cls-14 {
                font-family: 'Source Sans Pro', sans-serif;
                font-size: 15px;
              }
        
              .cls-4 {
                stroke: #8e24aa;
              }
        
              .cls-9 {
                font-size: 7.58px;
              }
        
              .cls-9, .cls-11 {
                font-family: ArialMT, Arial;
              }
        
              .cls-10 {
                fill: #8e24aa;
              }
        
              .cls-15 {
                letter-spacing: 0em;
              }
        
              .cls-11 {
                font-size: 6px;
              }
        
              .cls-5 {
                stroke: #43a047;
              }
        
              .cls-16 {
                letter-spacing: -.07em;
              }
        
              .cls-17 {
                letter-spacing: -.02em;
              }
        
              .cls-12 {
                fill: #fb8c00;
              }
        
              .cls-18 {
                letter-spacing: -.06em;
              }
        
              .cls-19 {
                letter-spacing: -.04em;
              }
        
              .cls-6 {
                stroke: #1e88e5;
              }
        
              .cls-13 {
                fill: #1e88e5;
              }
        
              .cls-14 {
                fill: #757575;
              }
            </style>
        <filter height="200%" id="glow" width="200%" x="-50%" y="-50%">
        <feGaussianBlur result="coloredBlur" stdDeviation="3.5"/>
        <feMerge>
        <feMergeNode in="coloredBlur"/>
        <feMergeNode in="SourceGraphic"/>
        </feMerge>
        </filter></defs>
        <path class="cls-6" d="M3,36.5h118.05"/>
        <path class="cls-1" d="M16.02,43.89c2.42,0,64.11.07,114.88.07,2.19-8.45,4.38-15.34,6.57-23.79h164.48"/>
        <line class="cls-5" x1="266.27" x2="422.31" y1="27.27" y2="27.27"/>
        <line class="cls-3" x1="212.62" x2="304.26" y1="72.97" y2="72.97"/>
        <line class="cls-4" x1="257.81" x2="448.84" y1="80.29" y2="80.29"/>
        <text class="cls-13" transform="translate(3 30.09)"><tspan x="0" y="0">Abstraction</tspan></text>
        <text class="cls-12" transform="translate(141.41 13.08)"><tspan x="0" y="0">Sketches</tspan></text>
        <text class="cls-8" transform="translate(307.81 21.66)"><tspan x="0" y="0">Ideation</tspan></text>
        <text class="cls-14" transform="translate(211.34 66.27)"><tspan x="0" y="0">Typography</tspan></text>
        <text class="cls-10" transform="translate(314.5 72.81)"><tspan x="0" y="0">Vector Graphics</tspan></text>
        <a href="https://clipasso.github.io/clipasso/" target="_blank">
        <g class="hover-glow">
        <rect class="cls-7" height="7.98" width="2.97" x="38.03" y="36.25"/>
        <circle class="cls-7" cx="39.55" cy="43.94" r="2.5"/>
        <text class="cls-9" transform="translate(25.22 55.87)"><tspan x="0" y="0">üèÜ CLIPasso</tspan></text>
        <text class="cls-11" transform="translate(24.79 62.94)"><tspan x="0" y="0">(SIGGRAPH '22)</tspan></text>
        <circle class="cls-7" cx="39.55" cy="36.5" r="2.5"/>
        </g></a>
        <a href="https://clipascene.github.io/CLIPascene/" target="_blank"><g>
        <text class="cls-9 hover-glow" transform="translate(25.42 74.53)"><tspan x="0" y="0">‚≠êÔ∏è CLIPascene</tspan></text>
        <text class="cls-11" transform="translate(25.79 81.33)"><tspan x="0" y="0">(ICCV '23)</tspan></text>
        </g></a>
        <a href="https://seva-benchmark.github.io/" target="_blank"><g>
        <rect class="cls-7" height="7.86" width="3.03" x="96.12" y="36.43"/>
        <circle class="cls-7" cx="97.64" cy="36.5" r="2.5"/>
        <text class="cls-9 hover-glow" transform="translate(84.71 55.72)"><tspan x="0" y="0">SEVA: Sketch </tspan><tspan x="0" y="9.09">Benchmark</tspan></text>
        <text class="cls-11" transform="translate(84.25 72.15)"><tspan x="0" y="0">(NeurIPS '23)</tspan></text>
        <circle class="cls-7" cx="97.64" cy="43.94" r="2.5"/>
        </g></a>
        <a href="https://swiftsketch.github.io/" target="_blank"><g>
        <circle class="cls-7" cx="157.48" cy="20.35" r="2.5"/>
        <text class="cls-9 hover-glow" transform="translate(143.14 31.99)"><tspan x="0" y="0">SwiftSketch</tspan></text>
        <text class="cls-11" transform="translate(143.14 38.1)"><tspan x="0" y="0">(SIGGRAPH '25)</tspan></text>
        </g></a>
        <a href="https://inklayer.github.io/" target="_blank"><g>
        <circle class="cls-7" cx="212.34" cy="20.35" r="2.5"/>
        <text class="cls-9 hover-glow" transform="translate(204.2 31.28)"><tspan x="0" y="0">InkLayer</tspan></text>
        <text class="cls-11" transform="translate(203.7 37.9)"><tspan x="0" y="0">(SIGGRAPH '25)</tspan></text>
        </g></a>
        <a href="https://sketch-agent.csail.mit.edu/" target="_blank"><g>
        <rect class="cls-2" height="9.23" width="3.1" x="284.48" y="19.35"/>
        <circle class="cls-7" cx="286.05" cy="27.59" r="2.5"/>
        <text class="cls-9 hover-glow" transform="translate(279.4 38.77)"><tspan x="0" y="0">SketchAgent</tspan></text>
        <text class="cls-11" transform="translate(279.4 44.89)"><tspan x="0" y="0">(CVPR '25)</tspan></text>
        <circle class="cls-7" cx="286.05" cy="20.17" r="2.5"/>
        </g></a>
        <a href="https://inspirationtree.github.io/inspirationtree/" target="_blank"><g>
        <circle class="cls-7" cx="399.98" cy="27.46" r="2.5"/>
        <text class="cls-9" transform="translate(374.51 38.44)"><tspan x="0" y="0">üèÜ Inspiration Tree</tspan></text>
        <text class="cls-11" transform="translate(374.51 45.56)"><tspan x="0" y="0">(SIGGRAPH</tspan><tspan class="cls-18" x="33.67" y="0"> </tspan><tspan x="35.01" y="0">Asia '23)</tspan></text>
        </g></a>
        <a href="https://wordasimage.github.io/Word-As-Image-Page" target="_blank"><g>
        <rect class="cls-7" height="7.48" width="3.08" x="285.92" y="72.81"/>
        <circle class="cls-7" cx="287.45" cy="80.29" r="2.5"/>
        <text class="cls-9 hover-glow" transform="translate(270.87 92.04)"><tspan class="cls-17" x="0" y="0">üèÜ Word-as-Image</tspan></text>
        <text class="cls-11" transform="translate(270.87 99.16)"><tspan x="0" y="0">(SIGGRAPH '23)</tspan></text>
        <circle class="cls-7" cx="287.45" cy="72.97" r="2.5"/>
        </g></a>
        <a href="https://livesketch.github.io/" target="_blank"><g>
        <circle class="cls-7" cx="354.92" cy="80.29" r="2.5"/>
        <text class="cls-9 hover-glow" transform="translate(338.96 91.56)"><tspan x="0" y="0">‚≠êÔ∏è Breathing Life </tspan><tspan x="0" y="9.09">Into Sketches</tspan></text>
        <text class="cls-11" transform="translate(338.26 107.79)"><tspan x="0" y="0">(CVPR '24)</tspan></text>
        </g></a>
        <a href="https://sagipolaczek.github.io/NeuralSVG/" target="_blank"><g>
        <circle class="cls-7" cx="424.81" cy="80" r="2.5"/>
        <text class="cls-9 hover-glow" transform="translate(417.69 92.08)"><tspan x="0" y="0">NeuralSVG</tspan></text>
        <text class="cls-11" transform="translate(417.69 98.19)"><tspan x="0" y="0">(ICCV '25)</tspan></text>
        </g></a>
        </svg>
          <div class="col-md-10 col-lg-10"></div>
      <div class="row mb-6">
        <div class="col-md-10 col-lg-10">
          <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal">
            <img class="img-fluid" src="img/portfolio/sketchseg_teaser.png" alt="">
          </div>
          <div class="portfolio-label">Instance Segmentation of Scene Sketches Using Natural Image Priors</div>
          <div class="portfolio-authors">Mia Tang, <highlight-name><b>Yael Vinker</b></highlight-name>, Chuan Yan, Lvmin Zhang, Maneesh Agrawala</div>
          <div class="portfolio-subtitle">SIGGRAPH 2025</div>

          <div class="portfolio-skill">
              <a style="padding-right: 1rem;" href="https://inklayer.github.io/" target="_blank"> > <highlight-secondary>Project website</highlight-secondary></a>
              <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2502.09608" target="_blank"> > <highlight-secondary> Paper</highlight-secondary></a>
              <a href="https://github.com/miatang13/InkLayer" target="_blank"> > <highlight-secondary> Code</highlight-secondary></a>
            </div>
        </div>

        <div class="col-md-10 col-lg-10">
          <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal">
            <img class="img-fluid" src="img/portfolio/teaser_swiftsketch.png" alt="">
          </div>
          <div class="portfolio-label">SwiftSketch: A Diffusion Model for Image-to-Vector Sketch Generation</div>
          <div class="portfolio-authors">Ellie Arar, Yarden Frenkel, Daniel Cohen-Or, Ariel Shamir, <highlight-name><b>Yael Vinker</b></highlight-name></div>
          <div class="portfolio-subtitle">SIGGRAPH 2025</div>

          <div class="portfolio-skill">
              <a style="padding-right: 1rem;" href="https://swiftsketch.github.io/" target="_blank"> > <highlight-secondary>Project website</highlight-secondary></a>
              <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2502.08642" target="_blank"> > <highlight-secondary> Paper</highlight-secondary></a>
              <a href="https://github.com/swiftsketch/SwiftSketch" target="_blank"> > <highlight-secondary> Code</highlight-secondary></a>
            </div>
        </div>

        <div class="col-md-10 col-lg-10">
          <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal">
            <img class="img-fluid" src="img/portfolio/teaser_sketchagent.jpg" alt="">
          </div>
          <div class="portfolio-label">SketchAgent: Language-Driven Sequential Sketch Generation</div>
          <div class="portfolio-authors"><highlight-name><b>Yael Vinker</b></highlight-name>, Tamar Rott Shaham, Kristine Zheng, Alex Zhao, Judith E Fan, Antonio Torralba</div>
          <div class="portfolio-subtitle">CVPR 2025</div>

          <div class="portfolio-skill">
              <a style="padding-right: 1rem;" href="https://yael-vinker.github.io/sketch-agent/" target="_blank"> > <highlight-secondary>Project website</highlight-secondary></a>
              <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2411.17673" target="_blank"> > <highlight-secondary> Paper</highlight-secondary></a>
              <a href="https://github.com/yael-vinker/SketchAgent" target="_blank"> > <highlight-secondary> Code</highlight-secondary></a>
            </div>
        </div>

        <div class="col-md-10 col-lg-10">
          <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal">
            <img class="img-fluid" src="img/portfolio/teaser_neuralsvg.png" alt="">
          </div>
          <div class="portfolio-label">NeuralSVG: An Implicit Representation for Text-to-Vector Generation</div>
          <div class="portfolio-authors">Sagi Polaczek, Yuval Alaluf, Elad Richardson, <highlight-name><b>Yael Vinker</b></highlight-name>, Daniel Cohen-Or</div>
          <div class="portfolio-subtitle">ICCV 2025</div>

          <div class="portfolio-skill">
              <a style="padding-right: 1rem;" href="https://sagipolaczek.github.io/NeuralSVG/" target="_blank"> > <highlight-secondary>Project website</highlight-secondary></a>
              <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2501.03992" target="_blank"> > <highlight-secondary> Paper</highlight-secondary></a>
              <a href="https://github.com/SagiPolaczek/NeuralSVG" target="_blank"> > <highlight-secondary> Code</highlight-secondary></a>
            </div>
        </div>

        

        <div class="col-md-10 col-lg-10">
          <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal">
            <img class="img-fluid" src="img/portfolio/teaser_style_content_blora.png" alt="">
          </div>
          <div class="portfolio-label">Implicit Style-Content Separation using B-LoRA</div>
          <div class="portfolio-authors">Yarden Frenkel, <highlight-name><b>Yael Vinker</b></highlight-name>, Ariel Shamir, Daniel Cohen-Or</div>
          <div class="portfolio-subtitle">ECCV 2024</div>

          <div class="portfolio-skill">
              <a style="padding-right: 1rem;" href="https://b-lora.github.io/B-LoRA/" target="_blank"> > <highlight-secondary>Project website</highlight-secondary></a>
              <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2403.14572" target="_blank"> > <highlight-secondary> Paper</highlight-secondary></a>
              <a href="https://github.com/yardenfren1996/B-LoRA" target="_blank"> > <highlight-secondary> Code</highlight-secondary></a>
            </div>
        </div>


        <div class="col-md-10 col-lg-10">

          

            <div class="portfolio-item portfolio-item-research mx-auto align-items-center" data-toggle="modal" data-target="#livesketch">
              
          <div class="row justify-content-center">

              <!-- <img class="img-fluid" src="img/portfolio/livesketch/livesketch_teaser.jpg" alt=""> -->
              <!-- <img class="img-fluid" style="max-width: 20%;" src="img/portfolio/livesketch/penguin.gif" alt="penguin"> -->
              <img class="img-fluid2" src="img/portfolio/livesketch/penguin.gif" alt="">
              <img class="img-fluid2" src="img/portfolio/livesketch/saxophone.gif" alt="">
              <img class="img-fluid2" src="img/portfolio/livesketch/fish.gif" alt="">
              <img class="img-fluid2" src="img/portfolio/livesketch/surfer2.gif" alt="">
            </div>
            
          </div>
            <div class="portfolio-label">Breathing Life Into Sketches Using Text-to-Video Priors</div>
            <div class="portfolio-authors"> Rinon Gal*, <highlight-name><b>Yael Vinker*</b></highlight-name>, Yuval Alaluf, Amit Bermano, Daniel Cohen-Or, Ariel Shamir, Gal Chechik</div>
            <div class="portfolio-subtitle">CVPR 2024, <highlight-award>Spotlight</highlight-award></div>

            <div class="portfolio-skill">
                <a style="padding-right: 1rem;" href="https://livesketch.github.io/" target="_blank"> > <highlight-secondary>Project website</highlight-secondary></a>
                <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2311.13608" target="_blank"> > <highlight-secondary> Paper</highlight-secondary></a>
                <a href="https://github.com/yael-vinker/live_sketch" target="_blank"> > <highlight-secondary> Code</highlight-secondary></a>
              </div>
        </div>

          <div class="col-md-10 col-lg-10">
            <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal">
              <img class="img-fluid" src="img/portfolio/omri_teaser.png" alt="">
            </div>
            <div class="portfolio-label">The Chosen One: Consistent Characters in Text-to-Image Diffusion Models</div>
            <div class="portfolio-authors"> Omri Avrahami, Amir Hertz, <highlight-name><b>Yael Vinker</b></highlight-name>, Moab Arar, Shlomi Fruchter, Ohad Fried, <br>Daniel Cohen-Or, Dani Lischinski</div>
            <div class="portfolio-subtitle">SIGGRAPH 2024</div>

            <div class="portfolio-skill">
                <a style="padding-right: 1rem;" href="https://omriavrahami.com/the-chosen-one/" target="_blank"> > <highlight-secondary>Project website</highlight-secondary></a>
                <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2311.10093" target="_blank"> > <highlight-secondary> Paper</highlight-secondary></a>
                <a href="https://github.com/ZichengDuan/TheChosenOne" target="_blank"> > <highlight-secondary> Code</highlight-secondary></a>
              </div>
          </div>

        <div class="col-md-10 col-lg-10">
          <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#InspirationTree">
            <img class="img-fluid" src="img/portfolio/inspirationTree_teaser.jpg" alt="">
          </div>
          <div class="portfolio-label">Concept Decomposition for Visual Exploration and Inspiration</div>
          <div class="portfolio-authors"><highlight-name><b>Yael Vinker</b></highlight-name>, Andrey Voynov, Daniel Cohen-Or, Ariel Shamir</div>
          <div class="portfolio-subtitle">SIGGRAPH Asia 2023, <highlight-award>Best Paper Award</highlight-award></div>

          <div class="portfolio-skill">
            <a style="padding-right: 1rem;" href="https://inspirationtree.github.io/inspirationtree/" target="_blank"> > <highlight-secondary>Project website</highlight-secondary></a>
            <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2305.18203" target="_blank"> > <highlight-secondary> Paper</highlight-secondary></a>
            <a href="https://github.com/google/inspiration_tree" target="_blank"> > <highlight-secondary> Code</highlight-secondary></a>
          </div>
        </div>

        <div class="col-md-10 col-lg-10">
            <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#seva">
              <img class="img-fluid" src="img/portfolio/seva_teaser.png" alt="">
            </div>
            <div class="portfolio-label">SEVA: Leveraging Sketches to Evaluate Alignment between Human and Machine Visual Abstraction</div>
            <div class="portfolio-authors">Kushin Mukherjee*, Holly Huey*, Xuanchen Lu*, <highlight-name><b>Yael Vinker</b></highlight-name>, Rio Aguina-Kang, Ariel Shamir, Judith E Fan</div>
            <div class="portfolio-subtitle">NeurIPS 2023 (Datasets and Benchmarks track)</div>
  
            <div class="portfolio-skill">
              <a style="padding-right: 1rem;" href="https://seva-benchmark.github.io/" target="_blank"> > <highlight-secondary>Project website</highlight-secondary></a>
              <a style="padding-right: 1rem;" href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=KhqjcM8AAAAJ&citation_for_view=KhqjcM8AAAAJ:HtEfBTGE9r8C" target="_blank"> > <highlight-secondary> Paper</highlight-secondary></a>
              <a href="https://github.com/cogtoolslab/visual_abstractions_benchmarking_public2023/" target="_blank"> > <highlight-secondary> Code</highlight-secondary></a>
            </div>
  
          </div>

        <div class="col-md-10 col-lg-10">
          <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#wordasimage">
            <img class="img-fluid" src="img/portfolio/word-as-image_teaser.png" alt="">
          </div>
          <div class="portfolio-label">Word-As-Image for Semantic Typography</div>
          <div class="portfolio-authors">Shir Iluz*, <highlight-name><b>Yael Vinker*</b></highlight-name>, Amir Hertz, Daniel Berio, Daniel Cohen-Or, Ariel Shamir</div>
          <div class="portfolio-subtitle">SIGGRAPH 2023, <highlight-award>Honorable Mention Award</highlight-award></div>

          <div class="portfolio-skill">
            <a style="padding-right: 1rem;" href="https://wordasimage.github.io/Word-As-Image-Page/" target="_blank"> > <highlight-secondary>Project website</highlight-secondary></a>
            <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2303.01818" target="_blank"> > <highlight-secondary> Paper</highlight-secondary></a>
            <a href="https://github.com/Shiriluz/Word-As-Image" target="_blank"> > <highlight-secondary> Code</highlight-secondary></a>
          </div>
        </div>

        <div class="row mb-6">
        <div class="col-md-10 col-lg-10">
          <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#attend&excite">
            <img class="img-fluid" src="img/portfolio/attend_and_excite_teaser.jpg" alt="">
          </div>
          <div class="portfolio-label">Attend-and-Excite: Attention-Based Semantic Guidance for <br>Text-to-Image Diffusion Models</div>
          <div class="portfolio-authors">Hila Chefer*, Yuval Alaluf*, <highlight-name><b>Yael Vinker</b></highlight-name>, Lior Wolf, Daniel Cohen-Or</div>
          <div class="portfolio-subtitle">SIGGRAPH 2023</div>

          <div class="portfolio-skill">
            <a style="padding-right: 1rem;" href="https://yuval-alaluf.github.io/Attend-and-Excite/" target="_blank"> > <highlight-secondary>Project website</highlight-secondary></a>
            <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2301.13826" target="_blank"> > <highlight-secondary> Paper</highlight-secondary></a>
            <a href="https://github.com/yuval-alaluf/Attend-and-Excite" target="_blank"> > <highlight-secondary> Code</highlight-secondary></a>
          </div>
        </div>


      <!-- <div class="row mb-6"> -->
        <div class="col-md-10 col-lg-10">
          <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#clipascene">
            
            <img class="img-fluid" src="img/portfolio/clipascene_teaser2.jpg" alt="">
          </div>
          <div class="portfolio-label">CLIPascene: Scene Sketching with Different Types <br> and Levels of Abstraction</div>
          <div class="portfolio-authors"><highlight-name><b>Yael Vinker</b></highlight-name>, Yuval Alaluf, Daniel Cohen-Or, Ariel Shamir</div>
          <div class="portfolio-subtitle">ICCV 2023, <highlight-award>Oral Presentation</highlight-award></div>

          <div class="portfolio-skill">
            <a style="padding-right: 1rem;" href="https://clipascene.github.io/CLIPascene/" target="_blank"> > <highlight-secondary>Project website</highlight-secondary></a>
            <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2211.17256" target="_blank"> > <highlight-secondary> Paper</highlight-secondary></a>
            <a href="https://github.com/yael-vinker/SceneSketch" target="_blank"> > <highlight-secondary> Code</highlight-secondary></a>
          </div>
        </div>        

      <!-- <div class="row mb-6"> -->
        <div class="col-md-10 col-lg-10">
          <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#clipasso">
            <div class="portfolio-item-caption h-100 w-100">
              <!-- <div class="portfolio-item-caption-content text-center">
                <p>Wheelchair detection and orientation classification for mobile robots.</p>
              </div> -->
            </div>
            <img class="img-fluid" src="img/portfolio/teaser2.png" alt="">
          </div>
          <div class="portfolio-label">CLIPasso: Semantically-Aware Object Sketching</div>
          <div class="portfolio-authors"><highlight-name><b>Yael Vinker</b></highlight-name>, Ehsan Pajouheshgar, Jessica Y. Bo, Roman Bachmann, <br>Amit Haim Bermano, Daniel Cohen-Or, Amir Zamir, Ariel Shamir</div>
          <div class="portfolio-subtitle">SIGGRAPH 2022, <highlight-award>Best Paper Award</highlight-award></div>

          <div class="portfolio-skill">
            <a style="padding-right: 1rem;" href="https://clipasso.github.io/clipasso/" target="_blank"> > <highlight-secondary>Project website</highlight-secondary></a>
            <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2202.05822" target="_blank"> > <highlight-secondary> Paper</highlight-secondary></a>
            <a href="https://github.com/yael-vinker/CLIPasso" target="_blank"> > <highlight-secondary> Code</highlight-secondary></a>
          </div>
        </div>
        
        <div class="col-md-10 col-lg-10">
          <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#deepsim">
            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
              <div class="portfolio-item-caption-content text-center">
                <!-- <p>Point cloud registration experiments with the HoloLens.</p> -->
              </div>
            </div>
            <img class="img-fluid" src="img/portfolio/deepsim.png" alt="">
          </div>
          <div class="portfolio-label">DeepSIM: Image Shape Manipulation from a Single <br> Augmented Training Sample</div>
          <div class="portfolio-authors"><highlight-name><b>Yael Vinker*</b></highlight-name>, Eliahu Horwitz*, Nir Zabari , Yedid Hoshen</div>
          <div class="portfolio-subtitle">ICCV 2021, <highlight-award>Oral Presentation</highlight-award></div>
        </div>

        <div class="col-md-10 col-lg-10">
          <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#hdr">
            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
              <div class="portfolio-item-caption-content text-center">
                <!-- <p>Point cloud registration experiments with the HoloLens.</p> -->
              </div>
            </div>
            <img class="img-fluid" src="research/hdr/teaser1_hdr.png" alt="">
          </div>
          <div class="portfolio-label">Unpaired Learning for High Dynamic Range Image Tone Mapping</div>
          <div class="portfolio-authors"><highlight-name><b>Yael Vinker</b></highlight-name>, Inbar Huberman-Spiegelglas, Raanan Fattal</div>
          <div class="portfolio-subtitle">ICCV 2021</div>
        </div>

    </div>
  </section>


  
  <div class="portfolio-modal modal fade" id="livesketch" tabindex="-1" role="dialog" aria-labelledby="livesketch" aria-hidden="true">
    <div class="modal-dialog modal-xl" role="document">
      <div class="modal-content">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">
            <i class="fas fa-times fa-xs"></i>
          </span>
        </button>
        <div class="modal-body">
          <div class="container">
            <div class="row justify-content-center">
              <div class="col-lg-10">
                <h1 class="portfolio-modal-title mb-4 text-center">Breathing Life Into Sketches Using Text-to-Video Priors</h1>
                <p class="font-italic mb-3">Rinon Gal*, <b>Yael Vinker*</b>, Yuval Alaluf, Amit Bermano, Daniel Cohen-Or, Ariel Shamir, Gal Chechik</p>
                
                <div class="video-container mb-4">
                  <video width="100%" height="auto" id="tree" controls>
                    <source src="img/portfolio/livesketch/main_video.mp4"
                    type="video/mp4">
                  </video>
                </div>

                <h6 class="large font-weight-bold"><highlight>Abstract: </highlight></h6>
                <p class="mb-3">
                    A sketch is one of the most intuitive and versatile tools humans use to convey their ideas visually. An animated sketch opens another dimension to the expression of ideas and is widely used by designers for a variety of purposes. Animating sketches is a laborious process, requiring extensive experience and professional design skills. In this work, we present a method that automatically adds motion to a single-subject sketch (hence, ``breathing life into it''), merely by providing a text prompt indicating the desired motion. The output is a short animation provided in vector representation, which can be easily edited. Our method does not require extensive training, but instead leverages the motion prior of a large pretrained text-to-video diffusion model using a score-distillation loss to guide the placement of strokes. To promote natural and smooth motion and to better preserve the sketch's appearance, we model the learned motion through two components. The first governs small local deformations and the second controls global affine transformations. Surprisingly, we find that even models that struggle to generate sketch videos on their own can still serve as a useful backbone for animating abstract representations.
                 </p>
                <div class="large mb-2 font-weight-bold">
                  <a href="https://livesketch.github.io/" target="_blank">
                    > <highlight>Project website</highlight>
                  </a>
                </div>

                <div class="large mb-2 font-weight-bold">
                  <a href="https://arxiv.org/abs/2311.13608" target="_blank">
                    > <highlight>Paper</highlight>
                  </a>
                </div>
                
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>


  
  <div class="portfolio-modal modal fade" id="seva" tabindex="-1" role="dialog" aria-labelledby="seva" aria-hidden="true">
    <div class="modal-dialog modal-xl" role="document">
      <div class="modal-content">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">
            <i class="fas fa-times fa-xs"></i>
          </span>
        </button>
        <div class="modal-body">
          <div class="container">
            <div class="row justify-content-center">
              <div class="col-lg-10">
                <h1 class="portfolio-modal-title mb-4 text-center">SEVA: Leveraging Sketches to Evaluate Alignment between Human and Machine Visual Abstraction</h1>
                <p class="font-italic mb-3">Kushin Mukherjee*, Holly Huey*, Xuanchen Lu*, <b>Yael Vinker</b>, Rio Aguina-Kang, Daniel Cohen-Or, Ariel Shamir, Judith E Fan</p>
                
                <div class="video-container mb-4">
                  <video width="100%" height="auto" id="tree" controls>
                    <source src="img/portfolio/seva_teaser.mp4"
                    type="video/mp4">
                  </video>
                </div>

                <h6 class="large font-weight-bold"><highlight>Abstract: </highlight></h6>
                <p class="mb-3"> Sketching is a powerful tool for creating abstract images that are sparse but meaningful. Sketch understanding poses fundamental challenges for general-purpose vision algorithms because it requires robustness to the sparsity of sketches relative to natural visual inputs and because it demands tolerance for semantic ambiguity, as sketches can reliably evoke multiple meanings. While current vision algorithms have achieved high performance on a variety of visual tasks, it remains unclear to what extent they understand sketches in a human-like way.

                    Here we introduce SEVA, a new benchmark dataset containing 90K human-generated sketches of 128 object concepts produced under different time constraints, and thus systematically varying in sparsity. We evaluated a suite of state-of-the-art vision algorithms on their ability to correctly identify the target concept depicted in these sketches and to generate responses that are strongly aligned with human response patterns on the same sketch recognition task. We found that vision algorithms that better predicted human sketch recognition performance also better approximated human uncertainty about sketch meaning, but there remains a sizable gap between model and human response patterns.
                    
                    To explore the potential of models that emulate human visual abstraction in generative tasks, we conducted further evaluations of a recently developed sketch generation algorithm (Vinker et al., 2022) capable of generating sketches that vary in sparsity. We hope that public release of this dataset and evaluation protocol will catalyze progress towards algorithms with enhanced capacities for human-like visual abstraction.
                 </p>
                <div class="large mb-2 font-weight-bold">
                  <a href="https://seva-benchmark.github.io/" target="_blank">
                    > <highlight>Project website</highlight>
                  </a>
                </div>

                <div class="large mb-2 font-weight-bold">
                  <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=KhqjcM8AAAAJ&citation_for_view=KhqjcM8AAAAJ:HtEfBTGE9r8C" target="_blank">
                    > <highlight>Paper</highlight>
                  </a>
                </div>
                
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>

<div class="portfolio-modal modal fade" id="InspirationTree" tabindex="-1" role="dialog" aria-labelledby="InspirationTree" aria-hidden="true">
    <div class="modal-dialog modal-xl" role="document">
      <div class="modal-content">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">
            <i class="fas fa-times fa-xs"></i>
          </span>
        </button>
        <div class="modal-body">
          <div class="container">
            <div class="row justify-content-center">
              <div class="col-lg-10">
                <h1 class="portfolio-modal-title mb-4 text-center">Concept Decomposition for Visual Exploration and Inspiration</h1>
                <p class="font-italic mb-3"><b>Yael Vinker</b>, Andrey Voynov, Daniel Cohen-Or, Ariel Shamir</p>
                
                <div class="video-container mb-4">
                  <video width="100%" height="auto" id="tree" controls>
                    <source src="img/portfolio/inspirationTree_gif.mp4"
                    type="video/mp4">
                  </video>
                </div>

                <h6 class="large font-weight-bold"><highlight>Abstract: </highlight></h6>
                <p class="mb-3">A creative idea is often born from transforming, combining, and modifying ideas from existing visual examples capturing various concepts. However, one cannot simply copy the concept as a whole, and inspiration is achieved by examining certain aspects of the concept. Hence, it is often necessary to separate a concept into different aspects to provide new perspectives. In this paper, we propose a method to decompose a visual concept, represented as a set of images, into different visual aspects encoded in a hierarchical tree structure. We utilize large vision-language models and their rich latent space for concept decomposition and generation. Each node in the tree represents a sub-concept using a learned vector embedding injected into the latent space of a pretrained text-to-image model. We use a set of regularizations to guide the optimization of the embedding vectors encoded in the nodes to follow the hierarchical structure of the tree. Our method allows to explore and discover new concepts derived from the original one. The tree provides the possibility of endless visual sampling at each node, allowing the user to explore the hidden sub-concepts of the object of interest. The learned aspects in each node can be combined within and across trees to create new visual ideas, and can be used in natural language sentences to apply such aspects to new designs.
                 </p>
                <div class="large mb-2 font-weight-bold">
                  <a href="https://inspirationtree.github.io/inspirationtree/" target="_blank">
                    > <highlight>Project website</highlight>
                  </a>
                </div>

                <div class="large mb-2 font-weight-bold">
                  <a href="https://arxiv.org/abs/2305.18203" target="_blank">
                    > <highlight>Paper</highlight>
                  </a>
                </div>
                
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>


<div class="portfolio-modal modal fade" id="wordasimage" tabindex="-1" role="dialog" aria-labelledby="wordasimage" aria-hidden="true">
    <div class="modal-dialog modal-xl" role="document">
      <div class="modal-content">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">
            <i class="fas fa-times fa-xs"></i>
          </span>
        </button>
        <div class="modal-body">
          <div class="container">
            <div class="row justify-content-center">
              <div class="col-lg-10">
                <h1 class="portfolio-modal-title mb-4 text-center">Word-As-Image for Semantic Typography</h1>
                <p class="font-italic mb-3">Shir Iluz*, <b>Yael Vinker*</b>, Amir Hertz, Daniel Berio, Daniel Cohen-Or, Ariel Shamir</p>
                
                <div class="video-container mb-4">
                  <video width="100%" height="auto" id="tree" controls>
                    <source src="img/portfolio/word-as-image-video.mp4"
                    type="video/mp4">
                  </video>
                </div>

                <h6 class="large font-weight-bold"><highlight>Abstract: </highlight></h6>
                <p class="mb-3">A word-as-image is a semantic typography technique where a word illustration presents a visualization of the meaning of the word, while also preserving its readability. We present a method to create word-as-image illustrations automatically. This task is highly challenging as it requires semantic understanding of the word and a creative idea of where and how to depict these semantics in a visually pleasing and legible manner. We rely on the remarkable ability of recent large pretrained language-vision models to distill textual concepts visually. We target simple, concise, black-and-white designs that convey the semantics clearly. We deliberately do not change the color or texture of the letters and do not use embellishments. Our method optimizes the outline of each letter to convey the desired concept, guided by a pretrained Stable Diffusion model. We incorporate additional loss terms to ensure the legibility of the text and the preservation of the style of the font. We show high quality and engaging results on numerous examples and compare to alternative techniques.
                 </p>
                <div class="large mb-2 font-weight-bold">
                  <a href="https://wordasimage.github.io/Word-As-Image-Page/" target="_blank">
                    > <highlight>Project website</highlight>
                  </a>
                </div>

                <div class="large mb-2 font-weight-bold">
                  <a href="https://arxiv.org/abs/2303.01818" target="_blank">
                    > <highlight>Paper</highlight>
                  </a>
                </div>
              
                <div class="large mb-2 font-weight-bold">
                  <a href="https://github.com/Shiriluz/Word-As-Image" target="_blank">
                    > <highlight>Code</highlight>
                  </a>
                </div>

                <div class="large mb-2 font-weight-bold">
                  <a href="https://huggingface.co/spaces/SemanticTypography/Word-As-Image" target="_blank">
                    > <highlight>Demo</highlight>
                  </a>
                </div>
                
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>


    <div class="portfolio-modal modal fade" id="attend&excite" tabindex="-1" role="dialog" aria-labelledby="attend&excite" aria-hidden="true">
    <div class="modal-dialog modal-xl" role="document">
      <div class="modal-content">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">
            <i class="fas fa-times fa-xs"></i>
          </span>
        </button>
        <div class="modal-body">
          <div class="container">
            <div class="row justify-content-center">
              <div class="col-lg-10">
                <h1 class="portfolio-modal-title mb-4 text-center">Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models</h1>
                <p class="font-italic mb-3">Hila Chefer*, Yuval Alaluf*, <b>Yael Vinker</b>, Lior Wolf, Daniel Cohen-Or</p>

                <div class="video-container mb-4">
                  <video width="100%" height="auto" id="tree" controls>
                    <source src="img/portfolio/attend_and_excite_vid.mp4"
                    type="video/mp4">
                  </video>
                </div>
                <h6 class="large font-weight-bold"><highlight>Abstract: </highlight></h6>
                <p class="mb-3">Recent text-to-image generative models have demonstrated an unparalleled ability to generate diverse and creative imagery guided by a target text prompt. While revolutionary, current state-of-the-art diffusion models may still fail in generating images that fully convey the semantics in the given text prompt. We analyze the publicly available Stable Diffusion model and assess the existence of catastrophic neglect, where the model fails to generate one or more of the subjects from the input prompt. Moreover, we find that in some cases the model also fails to correctly bind attributes (e.g. colors) to their corresponding subjects. To help mitigate these failure cases, we introduce the concept of Generative Semantic Nursing (GSN), where we seek to intervene in the generative process on the fly during inference time to improve the faithfulness of the generated images. Using an attention- based formulation of GSN, dubbed Attend-and-Excite, we guide the model to refine the cross-attention units to attend to all subject tokens in the text prompt and strengthen ‚Äî or excite ‚Äî their activations, encouraging the model to generate all subjects described in the text prompt. We compare our approach to alternative approaches and demonstrate that it conveys the desired concepts more faithfully across a range of text prompts.
                 </p>
                <div class="large mb-2 font-weight-bold">
                  <a href="https://yuval-alaluf.github.io/Attend-and-Excite/" target="_blank">
                    > <highlight>Project website</highlight>
                  </a>
                </div>

                <div class="large mb-2 font-weight-bold">
                  <a href="https://arxiv.org/abs/2301.13826" target="_blank">
                    > <highlight>Paper</highlight>
                  </a>
                </div>
              
                <div class="large mb-2 font-weight-bold">
                  <a href="https://github.com/yuval-alaluf/Attend-and-Excite" target="_blank">
                    > <highlight>Code</highlight>
                  </a>
                </div>
                
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>

    <div class="portfolio-modal modal fade" id="clipascene" tabindex="-1" role="dialog" aria-labelledby="clipascene" aria-hidden="true">
    <div class="modal-dialog modal-xl" role="document">
      <div class="modal-content">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">
            <i class="fas fa-times fa-xs"></i>
          </span>
        </button>
        <div class="modal-body">
          <div class="container">
            <div class="row justify-content-center">
              <div class="col-lg-10">
                <h1 class="portfolio-modal-title mb-4 text-center">CLIPascene: Scene Sketching with Different Types and Levels of Abstraction</h1>
                <p class="font-italic mb-3"><b>Yael Vinker</b>, Yuval Alaluf, Daniel Cohen-Or, Ariel Shamir</p>
                
                <div class="video-container mb-4">
                  <video width="100%" height="auto" id="tree" controls>
                    <source src="img/portfolio/clipascene_video1.mp4"
                    type="video/mp4">
                  </video>
                </div>

                <h6 class="large font-weight-bold"><highlight>Abstract: </highlight></h6>
                <p class="mb-3">In this paper, we present a method for converting a given scene image into a sketch using different types and multiple levels of abstraction. We distinguish between two types of abstraction.
                The first considers the fidelity of the sketch, varying its representation from a more precise portrayal of the input to a looser depiction.
                The second is defined by the visual simplicity of the sketch, moving from a detailed depiction to a sparse sketch.
                Using an explicit disentanglement into two abstraction axes - and multiple levels for each one - provides users additional control over selecting the desired sketch based on their personal goals and preferences.
                To form a sketch at a given level of fidelity and simplification, we train two MLP networks. The first network learns the desired placement of strokes, while the second network learns to gradually remove strokes from the sketch without harming its recognizability and semantics.
                Our approach is able to generate sketches of complex scenes including those with complex backgrounds (e.g., natural and urban settings) and subjects (e.g., animals and people) while depicting gradual abstractions of the input scene in terms of fidelity and simplicity.
                 </p>
                <div class="large mb-2 font-weight-bold">
                  <a href="https://clipascene.github.io/CLIPascene/" target="_blank">
                    > <highlight>Project website</highlight>
                  </a>
                </div>

                <div class="large mb-2 font-weight-bold">
                  <a href="https://arxiv.org/abs/2211.17256" target="_blank">
                    > <highlight>Paper</highlight>
                  </a>
                </div>
              
                <div class="large mb-2 font-weight-bold">
                  <a href="https://github.com/yael-vinker/SceneSketch" target="_blank">
                    > <highlight>Code</highlight>
                  </a>
                </div>
                
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>

  <div class="portfolio-modal modal fade" id="clipasso" tabindex="-1" role="dialog" aria-labelledby="wheelchairLabel" aria-hidden="true">
    <div class="modal-dialog modal-xl" role="document">
      <div class="modal-content">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">
            <i class="fas fa-times fa-xs"></i>
          </span>
        </button>
        <div class="modal-body">
          <div class="container">
            <div class="row justify-content-center">
              <div class="col-lg-10">
                <h1 class="portfolio-modal-title mb-4 text-center">CLIPasso: Semantically-Aware Object Sketching</h1>
                <p class="font-italic mb-3"><b>Yael Vinker</b>, Ehsan Pajouheshgar, Jessica Y. Bo, Roman Bachmann, Amit Haim Bermano, Daniel Cohen-Or, Amir Zamir, Ariel Shamir</p>
                
                <div class="video-container mb-4">
                  <video width="100%" height="auto" id="tree" controls>
                    <source src="img/portfolio/clipasso_vid.mp4"
                    type="video/mp4">
                  </video>
                </div>

                <h6 class="large font-weight-bold"><highlight>Abstract: </highlight></h6>
                <p class="mb-3">Abstraction is at the heart of sketching due to the simple and minimal nature of line drawings. Abstraction entails identifying the essential visual properties of an object or scene, which requires semantic understanding and prior knowledge of high-level concepts. Abstract depictions are therefore challenging for artists, and even more so for machines. We present an object sketching method that can achieve different levels of abstraction, guided by geometric and semantic simplifications. While sketch generation methods often rely on explicit sketch datasets for training, we utilize the remarkable ability of CLIP (Contrastive-Language-Image-Pretraining) to distill semantic concepts from sketches and images alike. We define a sketch as a set of B√©zier curves and use a differentiable rasterizer to optimize the parameters of the curves directly with respect to a CLIP-based perceptual loss. The abstraction degree is controlled by varying the number of strokes. The generated sketches demonstrate multiple levels of abstraction while maintaining recognizability, underlying structure, and essential visual components of the subject drawn. </p>
                <div class="large mb-2 font-weight-bold">
                  <a href="https://clipasso.github.io/clipasso/" target="_blank">
                    > <highlight>Project website</highlight>
                  </a>
                </div>

                <div class="large mb-2 font-weight-bold">
                  <a href="https://arxiv.org/abs/2202.05822" target="_blank">
                    > <highlight>Paper</highlight>
                  </a>
                </div>
              
                <div class="large mb-2 font-weight-bold">
                  <a href="https://github.com/yael-vinker/CLIPasso" target="_blank">
                    > <highlight>Code</highlight>
                  </a>
                </div>
                <div class="large mb-2 font-weight-bold">
                  <a href="https://replicate.com/yael-vinker/clipasso" target="_blank">
                    > <highlight>Demo</highlight>
                  </a>
                </div>
                
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>

  <div class="portfolio-modal modal fade" id="deepsim" tabindex="-1" role="dialog" aria-labelledby="visionLabel" aria-hidden="true">
    <div class="modal-dialog modal-xl" role="document">
      <div class="modal-content">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">
            <i class="fas fa-times fa-xs"></i>
          </span>
        </button>
        <div class="modal-body">
          <div class="container">
            <div class="row justify-content-center">
              <div class="col-lg-10">
                <h1 class="portfolio-modal-title mb-4 text-center">DeepSIM: Image Shape Manipulation from a Single Augmented Training Sample</h1>
                <p class="font-italic mb-3">Yael Vinker*, Eliahu Horwitz*, Nir Zabari , Yedid Hoshen</p>
                <img class="img-fluid rounded mb-3" src="img/portfolio/3dvision_img.png" alt="">
                
                
                <div class="video-container mb-4">
                  <iframe width="560" height="315" src="https://www.youtube.com/watch?v=RZwnnttQYzs&t=5s&ab_channel=EliahuHorwitz" frameborder="1" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                </div>

                <h6 class="large font-weight-bold"><highlight>Abstract: </highlight></h6>
                <p>We present DeepSIM, a generative model for conditional image manipulation based on a single image. We find that extensive augmentation is key for enabling single image training, and incorporate the use of thin-plate-spline (TPS) as an effective augmentation. Our network learns to map between a primitive representation of the image to the image itself. The choice of a primitive representation has an impact on the ease and expressiveness of the manipulations and can be automatic (e.g. edges), manual (e.g. segmentation) or hybrid such as edges on top of segmentations. At manipulation time, our generator allows for making complex image changes by modifying the primitive input representation and mapping it through the network. Our method is shown to achieve remarkable performance on image manipulation tasks.</p>

                <div class="large mb-2 font-weight-bold">
                  <a href="https://www.vision.huji.ac.il/deepsim/" target="_blank">
                    > <highlight>Project website</highlight>
                  </a>
                </div>

                <div class="large mb-2 font-weight-bold">
                  <a href="https://arxiv.org/pdf/2007.01289.pdf" target="_blank">
                    > <highlight>Paper</highlight>
                  </a>
                </div>
              
                <div class="large mb-2 font-weight-bold">
                  <a href="https://github.com/eliahuhorwitz/DeepSIM" target="_blank">
                    > <highlight>Code</highlight>
                  </a>
                </div>
                
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>


  <div class="portfolio-modal modal fade" id="hdr" tabindex="-1" role="dialog" aria-labelledby="visionLabel" aria-hidden="true">
    <div class="modal-dialog modal-xl" role="document">
      <div class="modal-content">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">
            <i class="fas fa-times fa-xs"></i>
          </span>
        </button>
        <div class="modal-body">
          <div class="container">
            <div class="row justify-content-center">
              <div class="col-lg-10">
                <h1 class="portfolio-modal-title mb-4 text-center">Unpaired Learning for High Dynamic Range Image Tone Mapping</h1>
                <p class="font-italic mb-3">Yael Vinker, Inbar Huberman-Spiegelglas, Raanan Fattal</p>
                <img class="img-fluid rounded mb-3" src="img/portfolio/3dvision_img.png" alt="">
                
                

                
                <div class="video-container mb-4">
                  <iframe width="560" height="315" src="https://youtu.be/v2r40TSRr3s" frameborder="1" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                </div>

                <h6 class="large font-weight-bold"><highlight>Abstract: </highlight></h6>
                <p>High dynamic range (HDR) photography is becoming increasingly popular and available by DSLR and mobile-phone cameras. While deep neural networks (DNN) have greatly impacted other domains of image manipulation, their use for HDR tone-mapping is limited due to the lack of a definite notion of ground-truth solution, which is needed for producing training data. In this paper we describe a new tone-mapping approach guided by the distinct goal of producing low dynamic range (LDR) renditions that best reproduce the visual characteristics of native LDR images. This goal enables the use of an unpaired adversarial training based on unrelated sets of HDR and LDR images, both of which are widely available and easy to acquire. In order to achieve an effective training under this minimal requirements, we introduce the following new steps and components: (i) a range-normalizing pre-process which estimates and applies a different level of curve-based compression, (ii) a loss that preserves the input content while allowing the network to achieve its goal, and (iii) the use of a more concise discriminator network, designed to promote the reproduction of low-level attributes native LDR possess. Evaluation of the resulting network demonstrates its ability to produce photo-realistic artifact-free tone-mapped images, and state-of-the-art performance on different image fidelity indices and visual distances.</p>

                <div class="large mb-2 font-weight-bold">
                  <a href="https://www.cs.huji.ac.il/w~raananf/projects/hdrgan/" target="_blank">
                    > <highlight>Project website</highlight>
                  </a>
                  <a href="https://arxiv.org/abs/2111.00219" target="_blank">
                    > <highlight>Paper</highlight>
                  </a>
                  <a href="https://github.com/yael-vinker/unpaired_hdr_tmo" target="_blank">
                    > <highlight>Code</highlight>
                  </a>
                </div>
                
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>



<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>


</body>

</html>
